{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e87fe83",
   "metadata": {},
   "source": [
    "# <center> Step 4: SQL Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f467ff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(filename='database_schema.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068cea58",
   "metadata": {},
   "source": [
    "## Loading necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fa63e3-cbf1-4224-a84f-42cdc626f5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "from sqlalchemy import create_engine, inspect\n",
    "from sqlalchemy import text\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f25533-5862-4c1b-b81b-c741bea3f25e",
   "metadata": {},
   "source": [
    "## Core connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63de7ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# SQL Config\n",
    "SQL_URL = os.getenv(\"SQL_URL\")\n",
    "\n",
    "SQL_ENGINE = create_engine(\n",
    "    SQL_URL,\n",
    "    connect_args={\"ssl\": {\"fake_flag_to_enable_tls\": True}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b6e64a",
   "metadata": {},
   "source": [
    "#### Creating Denormalized table with Primary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9873fb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the data into Python\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    c.city_id, c.city, m.pk_id, m.meeting_id,\n",
    "    mm.metric_id, mm.video_duration_sec, mm.item_count, mm.segment_count\n",
    "FROM cities c\n",
    "JOIN meetings m ON c.city_id = m.city_id\n",
    "JOIN meeting_metrics mm ON m.pk_id = mm.pk_id;\n",
    "\"\"\"\n",
    "\n",
    "with SQL_ENGINE.connect() as conn:\n",
    "    print(\"Fetching data...\")\n",
    "    df = pd.read_sql(text(query), conn)\n",
    "\n",
    "    # Manually create the table with a Primary Key\n",
    "    # We drop table if it exists first to ensure a clean slate\n",
    "    conn.execute(text(\"DROP TABLE IF EXISTS denormalized_table\"))\n",
    "    \n",
    "    create_statement = \"\"\"\n",
    "    CREATE TABLE denormalized_table (\n",
    "        metric_id BIGINT PRIMARY KEY,\n",
    "        city_id BIGINT,\n",
    "        city VARCHAR(255),\n",
    "        pk_id BIGINT,\n",
    "        meeting_id VARCHAR(255),\n",
    "        video_duration_sec BIGINT,\n",
    "        item_count BIGINT,\n",
    "        segment_count BIGINT\n",
    "    );\n",
    "    \"\"\"\n",
    "    print(\"Creating table with Primary Key...\")\n",
    "    conn.execute(text(create_statement))\n",
    "    conn.commit()\n",
    "\n",
    "    # Use to_sql to 'append' the data to the existing structure\n",
    "    print(f\"Uploading {len(df)} rows...\")\n",
    "    df.to_sql(\n",
    "        'denormalized_table', \n",
    "        con=conn, \n",
    "        if_exists='append', \n",
    "        index=False, \n",
    "        chunksize=500\n",
    "    )\n",
    "    conn.commit()\n",
    "\n",
    "print(\"Success! Denormalized table created with a Primary Key.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571bf47d",
   "metadata": {},
   "source": [
    "#### Reading tables from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e33848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_database(engine):\n",
    "    # Get all table names from the database\n",
    "    inspector = inspect(engine)\n",
    "    table_names = inspector.get_table_names()\n",
    "    \n",
    "    print(f\"Connected to Database. Found {len(table_names)} tables.\\n\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for table in table_names:\n",
    "        # Load table into a Pandas DataFrame\n",
    " \n",
    "        try:\n",
    "            df = pd.read_sql(f\"SELECT * FROM `{table}`\", engine)\n",
    "            \n",
    "            # Collect Details\n",
    "            rows, cols = df.shape\n",
    "            memory_usage = df.memory_usage(deep=True).sum() / 1024  # KB\n",
    "            \n",
    "            print(f\"TABLE: {table}\")\n",
    "            print(f\"  - Shape: {rows} rows x {cols} columns\")\n",
    "            print(f\"  - Size in Memory: {memory_usage:.2f} KB\")\n",
    "            print(f\"  - Columns & Types:\")\n",
    "            \n",
    "            # Displaying column names and types concisely\n",
    "            for col, dtype in df.dtypes.items():\n",
    "                print(f\"    - {col}: {dtype}\")\n",
    "                \n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading table {table}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inspect_database(SQL_ENGINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af34952",
   "metadata": {},
   "source": [
    "#### Printing sample from tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387d5291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of your table names\n",
    "tables = [\"cities\", \"meetings\", \"meeting_metrics\",\"denormalized_table\"]\n",
    "\n",
    "with SQL_ENGINE.connect() as conn:\n",
    "    for table in tables:\n",
    "        print(f\"--- Table: {table} (First few rows) ---\")\n",
    "        \n",
    "        # Query with limit - viewing a sample\n",
    "        query = text(f\"SELECT * FROM {table} LIMIT 5\")\n",
    "        df = pd.read_sql(query, conn)\n",
    "        \n",
    "        # Display the dataframe\n",
    "        display(df)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070196a6",
   "metadata": {},
   "source": [
    "#### Inefficient query example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c41572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_denormalized_inefficient():\n",
    "    # Drop the index if it exists\n",
    "    with SQL_ENGINE.connect() as conn:\n",
    "        try:\n",
    "            # Removing the index so MySQL has to do a Full Table Scan - \n",
    "            conn.execute(text(\"DROP INDEX idx_city_denormalized ON denormalized_table\"))\n",
    "            conn.commit()\n",
    "        except Exception:\n",
    "            pass \n",
    "\n",
    "    # Executing and timing the query\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    # Grouping by a non-indexed VARCHAR column\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        city,\n",
    "        AVG(video_duration_sec) AS avg_duration\n",
    "    FROM denormalized_table\n",
    "    GROUP BY city;\n",
    "    \"\"\"\n",
    "    \n",
    "    with SQL_ENGINE.connect() as conn:\n",
    "        df = pd.read_sql(text(query), conn)\n",
    "        \n",
    "    end_time = time.perf_counter()\n",
    "    return df, end_time - start_time\n",
    "\n",
    "df_bad_denorm, time_bad_denorm = run_denormalized_inefficient()\n",
    "print(f\"Inefficient Denormalized Runtime: {time_bad_denorm:.4f} seconds\")\n",
    "display(df_bad_denorm.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37334c83",
   "metadata": {},
   "source": [
    "#### Optimized query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d00292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimized_efficient():\n",
    "    # Recreating the test environment: Dropping and recreating the Index\n",
    "    with SQL_ENGINE.connect() as conn:\n",
    "        # Dropping if it exists to avoid a 'Duplicate key' error\n",
    "        try:\n",
    "            conn.execute(text(\"DROP INDEX idx_city_search ON cities\"))\n",
    "            conn.commit()\n",
    "        except Exception:\n",
    "            pass \n",
    "            \n",
    "        print(\"Creating index for optimization...\")\n",
    "        conn.execute(text(\"CREATE INDEX idx_city_search ON cities(city)\"))\n",
    "        conn.commit()\n",
    "            \n",
    "    # Executing and timing the query\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    # Aggregating on integers in a subquery to optimize efficiency\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        c.city,\n",
    "        sub.avg_duration\n",
    "    FROM cities c\n",
    "    JOIN (\n",
    "        SELECT \n",
    "            m.city_id, \n",
    "            AVG(mm.video_duration_sec) AS avg_duration\n",
    "        FROM meetings m\n",
    "        JOIN meeting_metrics mm ON m.pk_id = mm.pk_id\n",
    "        GROUP BY m.city_id\n",
    "    ) sub ON c.city_id = sub.city_id;\n",
    "    \"\"\"\n",
    "    \n",
    "    with SQL_ENGINE.connect() as conn:\n",
    "        df = pd.read_sql(text(query), conn)\n",
    "        \n",
    "    end_time = time.perf_counter()\n",
    "    return df, end_time - start_time\n",
    "\n",
    "df_good, time_good = run_optimized_efficient()\n",
    "print(f\"Optimized Efficient Runtime: {time_good:.4f} seconds\")\n",
    "display(df_good.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf83579",
   "metadata": {},
   "source": [
    "#### EXPLAIN ANALYZE querry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d08e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_denormalized_inefficient():\n",
    "    # Drop the index to force inefficiency for demonstration purposes\n",
    "    with SQL_ENGINE.connect() as conn:\n",
    "        try:\n",
    "            conn.execute(text(\"DROP INDEX idx_city_denormalized ON denormalized_table\"))\n",
    "            conn.commit()\n",
    "        except Exception:\n",
    "            pass \n",
    "\n",
    "    # EXPLAIN ANALYZE to the raw SQL\n",
    "    query = \"\"\"\n",
    "    EXPLAIN ANALYZE\n",
    "    SELECT \n",
    "        city,\n",
    "        AVG(video_duration_sec) AS avg_duration\n",
    "    FROM denormalized_table\n",
    "    GROUP BY city;\n",
    "    \"\"\"\n",
    "    \n",
    "    with SQL_ENGINE.connect() as conn:\n",
    "        # We use .scalar() or fetchone() because EXPLAIN ANALYZE returns a text block\n",
    "        result = conn.execute(text(query))\n",
    "        plan = result.fetchone()[0]\n",
    "        \n",
    "    return plan\n",
    "\n",
    "# Execute and print\n",
    "plan_bad = explain_denormalized_inefficient()\n",
    "print(\"--- Inefficient Query Plan ---\")\n",
    "print(plan_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287155ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_optimized_efficient():\n",
    "    # Ensure index exists for optimization\n",
    "    with SQL_ENGINE.connect() as conn:\n",
    "        try:\n",
    "            conn.execute(text(\"CREATE INDEX idx_city_search ON cities(city)\"))\n",
    "            conn.commit()\n",
    "        except Exception:\n",
    "            pass \n",
    "            \n",
    "    # EXPLAIN ANALYZE to the \"complex\" query\n",
    "    query = \"\"\"\n",
    "    EXPLAIN ANALYZE\n",
    "    SELECT \n",
    "        c.city,\n",
    "        sub.avg_duration\n",
    "    FROM cities c\n",
    "    JOIN (\n",
    "        SELECT \n",
    "            m.city_id, \n",
    "            AVG(mm.video_duration_sec) AS avg_duration\n",
    "        FROM meetings m\n",
    "        JOIN meeting_metrics mm ON m.pk_id = mm.pk_id\n",
    "        GROUP BY m.city_id\n",
    "    ) sub ON c.city_id = sub.city_id;\n",
    "    \"\"\"\n",
    "    \n",
    "    with SQL_ENGINE.connect() as conn:\n",
    "        result = conn.execute(text(query))\n",
    "        plan = result.fetchone()[0]\n",
    "        \n",
    "    return plan\n",
    "\n",
    "# Execute and print\n",
    "plan_good = explain_optimized_efficient()\n",
    "print(\"--- Optimized Query Plan ---\")\n",
    "print(plan_good)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0e39db",
   "metadata": {},
   "source": [
    "#### EXPLAIN ANALYZE Performance report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b669f173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_business_summary(plan_text, query_name):\n",
    "    # Extract the total time from the very first line (the root of the query)\n",
    "    time_match = re.search(r'actual time=\\d+\\.\\d+\\.\\.(\\d+\\.\\d+)', plan_text)\n",
    "    total_time = time_match.group(1) if time_match else \"Unknown\"\n",
    "    \n",
    "    # Extract how many rows the database *guessed* it needed to look at\n",
    "    rows_match = re.search(r'rows=(\\d+)', plan_text)\n",
    "    total_rows = rows_match.group(1) if rows_match else \"Unknown\"\n",
    "\n",
    "    # Look for key \"Red Flags\" or \"Green Flags\"\n",
    "    is_brute_force = \"Table scan\" in plan_text\n",
    "    \n",
    "    # Print the clean dashboard\n",
    "    print(f\"Performance Report: {query_name}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Total Time Taken : {total_time} milliseconds\")\n",
    "    print(f\"Data Volume Touched: ~{total_rows} rows\")\n",
    "    \n",
    "    if is_brute_force:\n",
    "        print(\"Strategy: 'Brute Force' (Full Table Scan)\")\n",
    "        print(\"Explanation: The database read every single row in the table like \")\n",
    "        print(\"reading a book cover-to-cover to find one word. This is fine for \")\n",
    "        print(\"small amounts of data, but will cause major lag as the system grows.\")\n",
    "    else:\n",
    "        print(\"Strategy: 'Surgical Precision' (Index Lookup)\")\n",
    "        print(\"Explanation: The database used an index (like a book's glossary) \")\n",
    "        print(\"to jump straight to the exact data needed. This will stay lightning \")\n",
    "        print(\"fast even if we add millions of rows to the system.\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f2bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_business_summary(plan_bad, \"Inefficient Query (No Indexes)\")\n",
    "print_business_summary(plan_good, \"Optimized Query (Indexed & Normalized)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc369ba2",
   "metadata": {},
   "source": [
    "## Advanced Query Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280d26c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_avg_segment_count_query():\n",
    "    # Recreating the test environment: Dropping and recreating the Index\n",
    "    with SQL_ENGINE.connect() as conn:\n",
    "        # Dropping if it exists to avoid a 'Duplicate key' error\n",
    "        try:\n",
    "            conn.execute(text(\"DROP INDEX idx_city_search ON cities\"))\n",
    "            conn.commit()\n",
    "        except Exception:\n",
    "            pass \n",
    "            \n",
    "        print(\"Creating index for optimization...\")\n",
    "        conn.execute(text(\"CREATE INDEX idx_city_search ON cities(city)\"))\n",
    "        conn.commit()\n",
    "            \n",
    "    # Executing and timing the query\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    # Using the CTE to calculate average segment count by city\n",
    "    query = \"\"\"\n",
    "    WITH city_segments AS (\n",
    "        SELECT \n",
    "            m.city_id,\n",
    "            AVG(mm.segment_count) AS avg_segments\n",
    "        FROM meetings m\n",
    "        JOIN meeting_metrics mm ON m.pk_id = mm.pk_id\n",
    "        GROUP BY m.city_id\n",
    "    )\n",
    "    SELECT \n",
    "        c.city, \n",
    "        cs.avg_segments\n",
    "    FROM cities c\n",
    "    JOIN city_segments cs ON c.city_id = cs.city_id;\n",
    "    \"\"\"\n",
    "    \n",
    "    with SQL_ENGINE.connect() as conn:\n",
    "        df = pd.read_sql(text(query), conn)\n",
    "        \n",
    "    end_time = time.perf_counter()\n",
    "    return df, end_time - start_time\n",
    "\n",
    "# Execute, unpack, and display the results\n",
    "df_segments, time_segments = run_avg_segment_count_query()\n",
    "print(f\"CTE Query Runtime: {time_segments:.4f} seconds\")\n",
    "display(df_segments.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc80a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_window_function_ranking():\n",
    "    with SQL_ENGINE.connect() as conn:\n",
    "        # Dropping if it exists to avoid a 'Duplicate key' error\n",
    "        try:\n",
    "            conn.execute(text(\"DROP INDEX idx_duration_sort ON meeting_metrics\"))\n",
    "            conn.commit()\n",
    "        except Exception:\n",
    "            pass \n",
    "            \n",
    "        print(\"Creating index on video_duration_sec for ranking optimization...\")\n",
    "        conn.execute(text(\"CREATE INDEX idx_duration_sort ON meeting_metrics(video_duration_sec DESC)\"))\n",
    "        conn.commit()\n",
    "            \n",
    "    # Executing and timing the query\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    # Executing the Window Function for Ranking\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        pk_id,\n",
    "        video_duration_sec,\n",
    "        item_count,\n",
    "        RANK() OVER (ORDER BY video_duration_sec DESC) AS duration_rank\n",
    "    FROM meeting_metrics;\n",
    "    \"\"\"\n",
    "    \n",
    "    with SQL_ENGINE.connect() as conn:\n",
    "        df = pd.read_sql(text(query), conn)\n",
    "        \n",
    "    end_time = time.perf_counter()\n",
    "    return df, end_time - start_time\n",
    "\n",
    "# Execute, unpack, and display the results\n",
    "df_ranked, time_ranked = run_window_function_ranking()\n",
    "print(f\"Window Function Runtime: {time_ranked:.4f} seconds\")\n",
    "display(df_ranked.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab10f461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analytical_top_meetings():\n",
    "    # Recreating the test environment: Indexing for Sort/Limit Performance\n",
    "    with SQL_ENGINE.connect() as conn:\n",
    "        # Dropping if it exists to avoid a 'Duplicate key' error\n",
    "        try:\n",
    "            conn.execute(text(\"DROP INDEX idx_item_count_sort ON meeting_metrics\"))\n",
    "            conn.commit()\n",
    "        except Exception:\n",
    "            pass \n",
    "            \n",
    "        print(\"Creating index on item_count for analytical optimization...\")\n",
    "        conn.execute(text(\"CREATE INDEX idx_item_count_sort ON meeting_metrics(item_count DESC)\"))\n",
    "        conn.commit()\n",
    "            \n",
    "    # Executing and timing the query\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    # Executing the Analytical Join Query\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        c.city,\n",
    "        m.meeting_id,\n",
    "        mm.video_duration_sec,\n",
    "        mm.item_count\n",
    "    FROM cities c\n",
    "    JOIN meetings m ON c.city_id = m.city_id\n",
    "    JOIN meeting_metrics mm ON m.pk_id = mm.pk_id\n",
    "    ORDER BY mm.item_count DESC\n",
    "    LIMIT 10;\n",
    "    \"\"\"\n",
    "    \n",
    "    with SQL_ENGINE.connect() as conn:\n",
    "        df = pd.read_sql(text(query), conn)\n",
    "        \n",
    "    end_time = time.perf_counter()\n",
    "    return df, end_time - start_time\n",
    "\n",
    "# Execute, unpack, and display the results\n",
    "df_analytics, time_analytics = run_analytical_top_meetings()\n",
    "print(f\"Analytical Query Runtime: {time_analytics:.4f} seconds\")\n",
    "display(df_analytics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89515e69",
   "metadata": {},
   "source": [
    "# <center> ----------------------Thank you :) ----------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
